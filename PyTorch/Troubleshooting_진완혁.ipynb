{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Troubleshooting_진완혁.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Out Of Memory(OOM)\n",
        "## OOM이 해결하기 어려운 이유들\n",
        "- 왜 발생했는지 알기 어렵다.\n",
        "- 어디서 발생했는지 알기 어렵다.\n",
        "- Error backtracking이 이상한데로 간다.\n",
        "- 메모리의 이전 상황의 파악이 어렵다.\n",
        "\n",
        "제일 간단한 방법 : batch size 줄이고 gpu clean 후 다시 돌리기\n",
        "\n",
        "### 그 외에 발생할 수 있는 문제가 있을 수 있다..\n",
        "\n",
        "# GRPUUtil 사용하기\n",
        "- nvidia-smi 처럼 GPU의 상태를 보여주는 모듈이다.\n",
        "- Colab은 환경에서 GPU 상태를 보여주기 편하다.\n",
        "- iter마다 메모리가 늘어나는지 확인!\n"
      ],
      "metadata": {
        "id": "mMK4lzKn_K6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "197tKHme-9XE",
        "outputId": "6931aba6-425b-4ef9-8278-bb44a9569b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=3453378f7f0d2f8b2aaa26856cc6fd24076f808dc3f115372f6fe6a9179f414f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install GPUtil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import GPUtil\n",
        "GPUtil.showUtilization()"
      ],
      "metadata": {
        "id": "taaZ7L3UA7QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# torch.cuda.empty_cache() 써보기\n",
        "- 사용되지 않는 GPU상 cache를 정리한다.\n",
        "- 가용 메모리를 확보한다.\n",
        "- del과는 구분이 필요하다.\n",
        "- reset 대신 쓰기 좋은 함수이다."
      ],
      "metadata": {
        "id": "q-JHnGflBQvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trainning loop에 tensor로 축적 되는 변수는 확인할 것\n",
        "- tensor로 처리된 변수는 GPU상에 메모리 사용\n",
        "- 해당 변수 loop 안에 연산에 있을 때 GPU에 computational graph를 생성(메모리 잠식)"
      ],
      "metadata": {
        "id": "zsAfe1buCIuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## del 명령어를 적절히 사용하기\n",
        "- 필요가 없어진 변수는 적절한 삭제가 필요하다.\n",
        "- python의 메모리 배치 특성상 loop이 끝나도 메모리를 차지한다."
      ],
      "metadata": {
        "id": "XbXwfzrYC19n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "'''\n",
        "for i in range(5):\n",
        "    intermediate = f(inputresult += g(intermediate)\n",
        "output = h(result)\n",
        "return output\n",
        "\n",
        "del intermediate\n",
        "'''\n"
      ],
      "metadata": {
        "id": "zOxtzHpVBCFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습시 OOM이 발생했다면 batch 사이즈를 1로 해서 실험해보기\n",
        "'''\n",
        "oom = False\n",
        "try:\n",
        "    run_model(batch_size)\n",
        "except RuntimeError: # out of memory\n",
        "    oom = True\n",
        "\n",
        "if oom:\n",
        "    for _ in range(batch_size):\n",
        "        run_model(1)\n",
        "'''"
      ],
      "metadata": {
        "id": "du5qZBuID0lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch.no_grad() 사용하기\n",
        "- inference 시점에서는 torch.no_grad() 구문을 사용한다.\n",
        "- backward pass로 인해 쌓이는 메모리에서 자유롭다."
      ],
      "metadata": {
        "id": "V9lr-ChYEGqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = network(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()"
      ],
      "metadata": {
        "id": "vig0kHHrEGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예상치 못한 에러 메세지\n",
        "- OOM 말고도 유사한 에러들이 발생한다.\n",
        "- CUDNN_STATUS_NOT_INIT 이나 device-side-assert 등\n",
        "- 해당 에러도 cuda와 관련하여 OOM의 일종으로 생각될 수 있으며, 적절한 코드 처리가 필요하다.\n",
        "\n",
        "# 그 외\n",
        "- colab에서 너무 큰 사이즈는 실행하지 말 것(linear, CNN, LSTM)\n",
        "- CNN의 대부분의 에러는 크기가 안 맞아서 생기는 경우(torchsummary 등으로 사이즈를 맞출 것)\n",
        "- tensor의 float precision을 16bit로 줄일 수도 이다."
      ],
      "metadata": {
        "id": "o4RpIR9-EX16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kzYaSZpyEjcM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}